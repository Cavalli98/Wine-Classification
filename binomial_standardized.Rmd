```{r}
rm(list=ls())
library("ggplot2")
library(patchwork)
library(rjags)
library(coda)
library(gridExtra)
source("utils_functions.R")
```

Loading the data
```{r}
wine = readRDS(file="./data/wine.Rda")
```

Pre processing data: remove outliers and scale data:
```{r}
df_cleaned <- wine
#df_cleaned <- remove_outliers(wine)
df <- as.data.frame(scale(df_cleaned[, 1:(ncol(df_cleaned) - 1)]))
df <- cbind(df, df_cleaned$quality)
names(df)[names(df) == "df_cleaned$quality"] <- "quality"
summary(df)
```

Divide starting set in training set and test set and setting the seed to a fixed value, so that the results are always the same.
```{r}
set.seed(144)
indexes = sample(1:nrow(df), size=0.5 * nrow(df))
train <- df[-indexes,]
test <- df[+indexes,]
ggplot() + 
  geom_histogram(data=train, aes(x=quality), bins=10, center=0, binwidth = 1, color=red_ex, fill=red_ex, alpha=0.4) +
  geom_histogram(data=test, aes(x=quality), bins=10, center=0, binwidth = 1, color=light_blue_ex, fill=light_blue_ex, alpha=0.4) + 
  xlim(c(0, 10)) 
```

Preparing data for Jags using appropriate data structures
```{r}
Y_train = as.vector(train$quality)
X_train = as.matrix(train[,!names(df) %in% c("quality"), drop=F])
```


Writing the model.
The first model we use, is a binomial likelihood with cumulative distribution function $\Phi()$ as link function 
and non informative gaussian prior over the regressors.
\begin{align*}
  y_i\mid \beta_0, \beta \sim \mathcal{B}i(\Phi(\beta_0 + \beta X_i))
  \beta_ \mid \lambda \sim \mathcal{N}(0, \lamdba^2)
  \beta \mid \lambda\sim \mathcal{N}(0, \lambda^2)
  \lambda^2 \sim \mathcal{I}\mathcal{G}a(1, 0.001)
\end{align*}

```{r}
string_binomial <- textConnection("model{
  # Likelihood
  for (i in 1:N){
    Y[i] ~ dbin(phi(beta0 + X[i,] %*% beta[]), 10)
  }
  # Prior
  beta0 ~ dnorm(0, 1.0 / lambda)
  for (i in 1: P){
    beta[i] ~ dnorm(0, 1.0 /lambda)
  }
  lambda ~ dgamma(1, 0.001)
}")
```

setting the hyperparameters of the model.
```{r}
N <- dim(X_train)[1]  # number of observations
P <- dim(X_train)[2]

data <-list(Y=Y_train, X=X_train, N=N, P=P)

burn     <- 1000
n.iter.update   <- 5000
n.chains <- 2
# Number of iterations & thinning
nit <- 50000
thin <-50
```
compiling the model
```{r}
binomial_model <- jags.model(string_binomial,data = data, n.chains=n.chains,quiet=FALSE)
```

updating the model
```{r}
cat("  Updating...\n")
update(binomial_model, n.iter=n.iter.update)

# Posterior parameters JAGS has to track
param <- c("beta0", "beta", "lambda")

```

sampling
```{r}
# Sampling 
cat("  Sampling...\n")
output <- coda.samples(model = binomial_model,
                       variable.names = param,
                       n.iter = nit,
                       thin = thin)
```
save the chain. Be sure to have a "chains" directory in your working directory
```{r}
save(output, file="chains/binomial_model_scaled.dat")
```
load the chain
```{r}
load(file="chains/binomial_model_scaled.dat")
```
Saving statistics as png
```{r}
s_object = summary(output)
stats_object = as.data.frame(s_object["statistics"])
new_names = names(wine)
new_names[ncol(wine)] = "intercept"
rownames(stats_object) <- new_names
stats_object
png("pictures/stats_gaussian.png", height = 50*nrow(stats_object), width =200*ncol(stats_object))
grid.table(stats_object)
dev.off()
```


Save the trace plots of the posterior distributions in a png file 
```{r}
ggsave("pictures/binomial/gaussian_traces_thin50.png", plot=ggplot_all_traces(output, 11))
```
Save the auto correlation plots in a png file
```{r}
ggsave("pictures/binomial/gaussian_corr_thin50.png", plot=ggplot_all_autocorr(output, 11, thinning=thin))
```


Save the densities plots in a png file
```{r}
p <- ggplot_all_densities(output, 11)
ggsave("pictures/binomial/gaussian_densities50.png", plot=p)
```

```{r}
beta <- as.matrix(output)[,1:12]
CI_beta = apply(beta, 2, quantile, c(0.025, 0.975))
```
```{r}
# Compute posterior mean
mean_beta_post <- apply(beta, 2, "mean")
mean_beta_post
```


```{r}
# PLOT THE CI
index = names(df)[1:11]
index <- append(index, "intercept")
index
ci_df <- data.frame(index=index, mean= mean_beta_post, lower=CI_beta[1,], upper=CI_beta[2,])
ci_df
p <- ggplot(ci_df, aes(index, mean)) +        
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper)) + 
  geom_hline(aes(yintercept = 0), color=red_ex)+
  theme(axis.text.x = element_text(angle = 90))
p
ggsave("pictures/binomial/CI_gaussian_.png", plot=p)
```

#Prediction
Take the data that has not been used to train the model and use them to make some predictions.
Store the samples of the rergressors to compute the predictive distribution of new data.
```{r}
Y_test = as.vector(test$quality)
X_test = as.matrix(test[,!names(test) %in% c("quality"), drop=F])

beta_0 <- as.matrix(output[,12])
betas <- as.matrix(output[, 1:11])
```
Here we compute for each sample of the test set the predictive distribution and we 
compare its posterior mean with the true label.
We use these comparisons to compute the MSE error.
We use the MSE error since this is an ordinal regression problem, thus, even though
the labels can be seen as classes, these classes can be ordered in a meaningful way and
so we are able to quantify the distance between two classes.
```{r}
error = 0
subN = length(Y_test)
missclassified = 0
n_iter = 500
y_pred <- numeric(length(Y_test))
pred_vs_true = matrix(nrow=length(Y_test), ncol=3)

for(i in 1:subN){
  Xnew = X_test[i,]
  predictive_distribution <- numeric(n_iter)
  for(g in 1:n_iter){ # loop over the iterations
    predictive_distribution[g] = pnorm(beta_0[g] + sum(Xnew*betas[g,]), mean = 0, sd = 1 )
  }
  y_pred[i] <- mean(predictive_distribution)
  missed = Y_test[i] - round(10 * y_pred[i])
  if (missed != 0){
      missclassified = missclassified + 1
  }
  eps = (Y_test[i] - (10 * y_pred[i]))
  error = error + (eps ** 2)
  # storing prediction and true value in data structure needed to plot the results
  pred_vs_true[i, 1] = i
  pred_vs_true[i, 2] = y_pred[i] * 10
  pred_vs_true[i, 3] = Y_test[i]
}
missclassified
subN
mse = error / subN
mse
```

Plotting portion of results
```{r}
results <-data.frame(
  "sample"=pred_vs_true[100:200,1],
  "prediction"=pred_vs_true[100:200, 2],
  "true_value"=pred_vs_true[100:200, 3]
  )


plt_results <-ggplot(data = results, mapping = aes(x = sample )) +
    geom_hline(aes(yintercept = 0)) +
    geom_segment(mapping = aes(xend =sample, y=true_value, yend = 0), size=1.5, color=red_ex, alpha=0.4) + 
    geom_segment(mapping = aes(xend =sample, y=prediction, yend = 0), size=1.5, color=light_blue_ex, alpha=0.4) + 
    scale_y_continuous(name="values", limits=c(0, 10))

plt_results
ggsave("pictures/binomial/gaussian_results.png", plot=plt_results)

```

# Lasso
Writing the model.
Now we try to embed the feature selection in the prior distribution of the parameters by 
using a lasso prior:
\begin{align*}
  y_i \mid \beta_0, e\beta \sim \mathcal{B}i(\Phi(\beta_0 + \beta X_i))
  \beta_0 \mid \lambda \sim \mathcal{L}aplace(0, \lambda^2)
  \beta \mid \lambda \sim \mathcal{L}aplace(0, \lambda^2)
  \lambda^2 \sim \mathcal{I}\mathcal{G}(\alpha, \beta)
\end{align*}
We try different values for \alpha and \beta
Basically we do the same stuff as before.

```{r}
string_binomial <- textConnection("model{
  # Likelihood
  for (i in 1:N){
    Y[i] ~ dbin(phi(beta0 + X[i,] %*% beta[]), 10)
  }
  # Prior
  beta0 ~ ddexp(0, 1.0 / lambda)
  for (i in 1: P){
    beta[i] ~ ddexp(0, 1.0 / lambda)
  }
  lambda ~ dgamma(0.1, 0.1)
}")
```

```{r}
N <- dim(X_train)[1]  # number of observations
P <- dim(X_train)[2]

data <-list(Y=Y_train, X=X_train, N=N, P=P)

burn     <- 1000
n.iter.update   <- 5000
n.chains <- 2
# Number of iterations & thinning
nit <- 50000
thin <-50
```

```{r}
binomial_model <- jags.model(string_binomial,data = data, n.chains=n.chains,quiet=FALSE)
```

```{r}
cat("  Updating...\n")
update(binomial_model, n.iter=n.iter.update)

# Posterior parameters JAGS has to track
param <- c("beta0", "beta", "lambda")

```

```{r}
# Sampling 
cat("  Sampling...\n")
output <- coda.samples(model = binomial_model,
                       variable.names = param,
                       n.iter = nit,
                       thin = thin)
```


```{r}
save(output, file="chains/binomial_model_scaled_lasso_thin50_alpha01.dat")
```

```{r}
load(file="chains/binomial_model_scaled_lasso_thin50_alpha01.dat")
```

```{r}

s_object = summary(output)
stats_object = as.data.frame(s_object["statistics"])
new_names = names(wine)
new_names[ncol(wine)] = "intercept"
new_names <- append(new_names, "lambda")
rownames(stats_object) <- new_names
stats_object
png("pictures/stats_lasso01.png", height = 50*nrow(stats_object), width =200*ncol(stats_object))
grid.table(stats_object)
dev.off()

```

```{r}
ggsave("pictures/binomial/lasso_traces_thin50_alpha01.png", plot=ggplot_all_traces(output, 11))
```
```{r}
ggsave("pictures/binomial/lasso_corr_thin50_alpha01.png", plot=ggplot_all_autocorr(output, 11, thinning=thin))
```


```{r}
ggsave("pictures/binomial/lasso_density_thin50_alpha01.png", plot=ggplot_all_densities(output, 11))
```


inv.var
```{r}
p <- ggplot_density_MC(output, "lambda")
ggsave("pictures/binomial/lambda_density_thin50_alpha01.png", plot=p)
p <- ggplot_traceplot(output, "lambda")
ggsave("pictures/binomial/lambda_trace_thin50_alpha01.png", plot=p)
p <- ggplot_autocorr(output, "lambda", thinning = thin(output))
ggsave("pictures/binomial/lambda_corr_thin50_alpha01.png", plot=p)
```
confidence intervals for lasso
```{r}
beta <- as.matrix(output)[,1:12]
CI_beta = apply(beta, 2, quantile, c(0.025, 0.975))
```
```{r}

# For loop to check the included variables
idx_cov_BL = NULL
names_kept = c()
for(l in 1:P){
  # 0 is inside the confidence interval
  if(CI_beta[1,l]<0 && CI_beta[2,l]>0) {
    cat("*** variable ", colnames(df)[l], " excluded \n")
  }
  # 0 is outside the confidence interval
  else {
    cat("*** variable ", colnames(df)[l], " included \n")
    idx_cov_BL = c(idx_cov_BL, l)
    names_kept = c(names_kept, colnames(df)[l])
  }
}

cat("\nMeaningful variables selected using a credible interval of 95% is:\n", names_kept)
```

```{r}
# Compute posterior mean
mean_beta_post <- apply(beta, 2, "mean")
mean_beta_post
```


```{r}
# PLOT THE CI
index = names(df)[1:11]
index <- append(index, "intercept")
index
ci_df <- data.frame(index=index, mean= mean_beta_post, lower=CI_beta[1,], upper=CI_beta[2,])
ci_df
p <- ggplot(ci_df, aes(index, mean)) +        
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper)) + 
  geom_hline(aes(yintercept = 0), color=red_ex)+
  theme(axis.text.x = element_text(angle = 90))
p
ggsave("pictures/binomial/CI_lasso_thin50_alpha01.png", plot=p)

```

# Lasso much shrinked
Writing the model.
Here we just change the prior over the variance for the Laplace distribution and 
compare the results with the ones got with the other prior.

```{r}
string_binomial <- textConnection("model{
  # Likelihood
  for (i in 1:N){
    Y[i] ~ dbin(phi(beta0 + X[i,] %*% beta[]), 10)
  }
  # Prior
  beta0 ~ ddexp(0, 1.0 / lambda)
  for (i in 1: P){
    beta[i] ~ ddexp(0, 1.0 / lambda)
  }
  lambda ~ dgamma(0.5, 0.1)
}")
```

```{r}
N <- dim(X_train)[1]  # number of observations
P <- dim(X_train)[2]

data <-list(Y=Y_train, X=X_train, N=N, P=P)

burn     <- 1000
n.iter.update   <- 5000
n.chains <- 2
# Number of iterations & thinning
nit <- 50000
thin <-50
```

```{r}
binomial_model <- jags.model(string_binomial,data = data, n.chains=n.chains,quiet=FALSE)
```

```{r}
cat("  Updating...\n")
update(binomial_model, n.iter=n.iter.update)

# Posterior parameters JAGS has to track
param <- c("beta0", "beta", "lambda")

```

```{r}
# Sampling 
cat("  Sampling...\n")
output <- coda.samples(model = binomial_model,
                       variable.names = param,
                       n.iter = nit,
                       thin = thin)
```


```{r}
save(output, file="chains/binomial_model_scaled_lasso_thin50_alpha05.dat")
```

```{r}
load(file="chains/binomial_model_scaled_lasso_thin50_alpha05.dat")
```

```{r}
s_object = summary(output)
stats_object = as.data.frame(s_object["statistics"])
new_names = names(wine)
new_names[ncol(wine)] = "intercept"
new_names <- append(new_names, "lambda")
rownames(stats_object) <- new_names
stats_object
png("pictures/stats_lasso05.png", height = 50*nrow(stats_object), width =200*ncol(stats_object))
grid.table(stats_object)
dev.off()

```

```{r}
ggsave("pictures/binomial/lasso_traces_thin50_alpha05.png", plot=ggplot_all_traces(output, 11))
```
```{r}
ggsave("pictures/binomial/lasso_corr_thin50_alpha05.png", plot=ggplot_all_autocorr(output, 11, thinning=thin))
```


```{r}
ggsave("pictures/binomial/lasso_density_thin50_alpha05.png", plot=ggplot_all_densities(output, 11))
```

inv.var
```{r}
ggplot_density_MC(output, "lambda")
ggplot_traceplot(output, "lambda")
ggplot_autocorr(output, "lambda", thinning = thin(output))
ggsave("pictures/binomial/lambda_lasso_alpha05.png", plot= ggplot_density_MC(output, "lambda"))
```
confidence intervals for lasso
```{r}
beta <- as.matrix(output)[,1:12]
CI_beta = apply(beta, 2, quantile, c(0.025, 0.975))
```
```{r}

# For loop to check the included variables
idx_cov_BL = NULL
names_kept = c()
for(l in 1:P){
  # 0 is inside the confidence interval
  if(CI_beta[1,l]<0 && CI_beta[2,l]>0) {
    cat("*** variable ", colnames(df)[l], " excluded \n")
  }
  # 0 is outside the confidence interval
  else {
    cat("*** variable ", colnames(df)[l], " included \n")
    idx_cov_BL = c(idx_cov_BL, l)
    names_kept = c(names_kept, colnames(df)[l])
  }
}

cat("\nMeaningful variables selected using a credible interval of 95% is:\n", names_kept)
```

```{r}
# Compute posterior mean
mean_beta_post <- apply(beta, 2, "mean")
mean_beta_post
```


```{r}
# PLOT THE CI
index = names(df)[1:11]
index <- append(index, "intercept")
index
ci_df <- data.frame(index=index, mean= mean_beta_post, lower=CI_beta[1,], upper=CI_beta[2,])
ci_df
p <- ggplot(ci_df, aes(index, mean)) +        
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper)) + 
  geom_hline(aes(yintercept = 0), color=red_ex)+
  theme(axis.text.x = element_text(angle = 90))
p
ggsave("pictures/binomial/CI_lasso_alpha05.png", plot=p)
```

## Lasso with another prior


```{r}
string_binomial <- textConnection("model{
  # Likelihood
  for (i in 1:N){
    Y[i] ~ dbin(phi(beta0 + X[i,] %*% beta[]), 10)
  }
  # Prior
  beta0 ~ ddexp(0, 1.0 / lambda)
  for (i in 1: P){
    beta[i] ~ ddexp(0, 1.0 / lambda)
  }
  lambda ~ dgamma(0.01, 0.1)
}")
```

```{r}
N <- dim(X_train)[1]  # number of observations
P <- dim(X_train)[2]

data <-list(Y=Y_train, X=X_train, N=N, P=P)

burn     <- 1000
n.iter.update   <- 5000
n.chains <- 2
# Number of iterations & thinning
nit <- 50000
thin <-50
```

```{r}
binomial_model <- jags.model(string_binomial,data = data, n.chains=n.chains,quiet=FALSE)
```

```{r}
cat("  Updating...\n")
update(binomial_model, n.iter=n.iter.update)

# Posterior parameters JAGS has to track
param <- c("beta0", "beta", "lambda")

```

```{r}
# Sampling 
cat("  Sampling...\n")
output <- coda.samples(model = binomial_model,
                       variable.names = param,
                       n.iter = nit,
                       thin = thin)
```


```{r}
save(output, file="chains/binomial_model_scaled_lasso_thin50_alpha001.dat")
```
```{r}
load(file="chains/binomial_model_scaled_lasso_thin50_alpha001.dat")
```

```{r}
s_object = summary(output)
stats_object = as.data.frame(s_object["statistics"])
new_names = names(wine)
new_names[ncol(wine)] = "intercept"
new_names <- append(new_names, "lambda")
rownames(stats_object) <- new_names
stats_object
png("pictures/stats_lasso001.png", height = 50*nrow(stats_object), width =200*ncol(stats_object))
grid.table(stats_object)
dev.off()

```

```{r}
ggsave("pictures/binomial/lasso_traces_thin50_alpha001.png", plot=ggplot_all_traces(output, 11))
```
```{r}
ggsave("pictures/binomial/lasso_corr_thin50_alpha001.png", plot=ggplot_all_autocorr(output, 11, thinning=thin))
```


```{r}
ggsave("pictures/binomial/lasso_density_thin50_alpha001.png", plot=ggplot_all_densities(output, 11))
```


confidence intervals for lasso
```{r}
beta <- as.matrix(output)[,1:12]
CI_beta = apply(beta, 2, quantile, c(0.025, 0.975))
```
```{r}

# For loop to check the included variables
idx_cov_BL = NULL
names_kept = c()
for(l in 1:P){
  # 0 is inside the confidence interval
  if(CI_beta[1,l]<0 && CI_beta[2,l]>0) {
    cat("*** variable ", colnames(df)[l], " excluded \n")
  }
  # 0 is outside the confidence interval
  else {
    cat("*** variable ", colnames(df)[l], " included \n")
    idx_cov_BL = c(idx_cov_BL, l)
    names_kept = c(names_kept, colnames(df)[l])
  }
}

cat("\nMeaningful variables selected using a credible interval of 95% is:\n", names_kept)
```

```{r}
# Compute posterior mean
mean_beta_post <- apply(beta, 2, "mean")
mean_beta_post
```




```{r}
names(df)[1:11]
index = names(df)[1:11]
index <- append(index, "intercept")
index
ci_df <- data.frame(index=index, mean= mean_beta_post, lower=CI_beta[1,], upper=CI_beta[2,])
ci_df
p <- ggplot(ci_df, aes(index, mean)) +        
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper)) + 
  geom_hline(aes(yintercept = 0), color=red_ex)+
  theme(axis.text.x = element_text(angle = 90))
p
ggsave("pictures/binomial/CI_lasso_alpha001.png", plot=p)
```


# Comparison between the different lassos priors
```{r}
load(file="chains/binomial_model_scaled_lasso_thin50_alpha001.dat")
alpha001 = output
load(file="chains/binomial_model_scaled_lasso_thin50_alpha01.dat")
alpha01 = output
load(file="chains/binomial_model_scaled_lasso_thin50_alpha05.dat")
alpha05 = output


p <- ggplot_compare_all_d(alpha01, alpha05, 11)
p
ggsave("pictures/binomial/binomial_comparison_01_vs_05.png", plot=p)


p <- ggplot_compare_all_d(alpha05, alpha001, 11)
p
ggsave("pictures/binomial/binomial_comparison_05_vs_001.png", plot=p)

```


# comparison between the different lasso priors:
colors of posterior distributions:
- red -> alpha =0.01
-green -> alpha = 0.1
- blue -> alpha = 0.5
```{r}
transparency = 0.3
betas.names = get_beta_names(1, 11)
densities = ggplot() + 
  geom_density(data = data.frame(alpha001[[1]]), aes_string(x="beta0"), color=red_ex, fill=red_ex, alpha=transparency) +
  geom_density(data = data.frame(alpha05[[1]]), aes_string(x="beta0"), color=light_blue_ex, fill=light_blue_ex, alpha=transparency) +
  geom_density(data = data.frame(alpha01[[1]]), aes_string(x="beta0"), color=green_ex, fill=green_ex, alpha=transparency)
for (i in 1:11){
  new_beta = ggplot() + 
  geom_density(data = data.frame(alpha001[[1]]), aes_string(x=betas.names[i]), color=red_ex, fill=red_ex, alpha=transparency
               ) +
  geom_density(data = data.frame(alpha05[[1]]), aes_string(x=betas.names[i]), color=light_blue_ex, fill=light_blue_ex, alpha=transparency) +
  geom_density(data = data.frame(alpha01[[1]]), aes_string(x=betas.names[i]), color=green_ex, fill=green_ex, alpha=transparency)
  densities = densities + new_beta
}
densities = densities +  plot_layout(ncol = 3)
densities
ggsave("pictures/binomial/lasso_posterior_comparisons.png", plot=densities)
```

# comparison between the different priors over lasso:
colors of lambda:
- red -> alpha =0.01
-green -> alpha = 0.1
- blue -> alpha = 0.5

```{r}
green_ex = "#8ac926"
p <- ggplot() + 
  geom_density(data = data.frame(alpha001[[1]]), aes_string(x="lambda"), color=red_ex, fill=red_ex, alpha=0.5) +
  geom_density(data = data.frame(alpha05[[1]]), aes_string(x="lambda"), color=light_blue_ex, fill=light_blue_ex, alpha=0.5) +
  geom_density(data = data.frame(alpha01[[1]]), aes_string(x="lambda"), color=green_ex, fill=green_ex, alpha=0.5)

ggsave("pictures/binomial/binomial_comparison_lasso_lambdas_red_blue_green.png", plot=p)
```

#Comparison between posteriors gotten using a Gaussian and a Lasso
We just choose the Lasso with \alpha = \beta = 0.1,  Since the posterior distributions using the prior Lasso are very similar(when changing the prior over the Lasso)
```{r}
load(file="chains/binomial_model_scaled.dat")
gaussian = output
load(file="chains/binomial_model_scaled_lasso_thin50_alpha01.dat")
lasso = output
p <- ggplot_compare_all_d(gaussian, lasso, 11)
p
ggsave("pictures/comparison_gaussian_lasso.png", plot=p)
```




# Prediction using the Lasso.
Since the posterior distributions using the prior Lasso are very similar(when changing the prior over the Lasso), we can make prediction with just one of them for sake of simplicity.
We just choose the one with \alpha = \beta = 0.1.
First, take the data that has not been used to train the model and use them to make some predictions.
Store the samples of the rergressors to compute the predictive distribution of new data.

```{r}
Y_test = as.vector(test$quality)
X_test = as.matrix(test[,!names(test) %in% c("quality"), drop=F])
#load the chain of the model we want to use
load(file="chains/binomial_model_scaled_lasso_thin50_alpha01.dat")
beta_0 <- as.matrix(output[,12])
betas <- as.matrix(output[, 1:11])
```

We use these comparisons to compute the MSE error.
We use the MSE error since this is an ordinal regression problem, thus, even though
the labels can be seen as classes, these classes can be ordered in a meaningful way and
so we are able to quantify the distance between two classes.
```{r}

error = 0
subN = length(Y_test)
missclassified = 0
n_iter = 500
y_pred <- numeric(length(Y_test))
pred_vs_true = matrix(nrow=length(Y_test), ncol=3)

for(i in 1:subN){
  Xnew = X_test[i,]
  predictive_distribution <- numeric(n_iter)
  for(g in 1:n_iter){ # loop over the iterations
    predictive_distribution[g] = pnorm(beta_0[g] + sum(Xnew*betas[g,]), mean = 0, sd = 1 )
  }
  y_pred[i] <- mean(predictive_distribution)
  missed = Y_test[i] - round(10 * y_pred[i])
  if (missed != 0){
      missclassified = missclassified + 1
  }
  eps = (Y_test[i] - (10 * y_pred[i]))
  error = error + (eps ** 2)
  # storing prediction and true value in data structure needed to plot the results
  pred_vs_true[i, 1] = i
  pred_vs_true[i, 2] = y_pred[i] * 10
  pred_vs_true[i, 3] = Y_test[i]
}
missclassified
subN
mse = error / subN
mse
```

Plotting portion of results
```{r}
results <-data.frame(
  "sample"=pred_vs_true[100:200,1],
  "prediction"=pred_vs_true[100:200, 2],
  "true_value"=pred_vs_true[100:200, 3]
  )


plt_results <-ggplot(data = results, mapping = aes(x = sample )) +
    geom_hline(aes(yintercept = 0)) +
    geom_segment(mapping = aes(xend =sample, y=true_value, yend = 0), size=1.5, color=red_ex, alpha=0.4) + 
    geom_segment(mapping = aes(xend =sample, y=prediction, yend = 0), size=1.5, color=light_blue_ex, alpha=0.4) + 
    scale_y_continuous(name="values", limits=c(0, 10))

plt_results
ggsave("pictures/binomial/lasso_results.png", plot=plt_results)

```

